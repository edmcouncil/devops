#!/usr/bin/env groovy
//
// The main Jenkinsfile for FIBO, defining the Build/Publish/Test/Deploy process that is
// executed for each push into the repository.
//
// Note that this file is in the so called "Declarative Pipeline" syntax
//
// See https://jenkins.io/doc/book/pipeline/jenkinsfile/
//
// Required plugins:
// - [Pipeline: SCM Step](https://plugins.jenkins.io/workflow-scm-step/) = workflow-scm-step
// - [Pipeline Utility Steps](https://plugins.jenkins.io/pipeline-utility-steps/) = pipeline-utility-steps

//	https://www.jvt.me/posts/2020/02/23/jenkins-multibranch-skip-branch-index/
if (currentBuild.getBuildCauses().toString().contains('BranchIndexingCause')) {
  print "INFO: Build skipped due to trigger being Branch Indexing"
  currentBuild.result = 'SUCCESS'
  return
}

import groovy.json.JsonSlurper
import groovy.transform.Field

//String[] derivedProducts = ['datadictionary', 'index', 'vocabulary']
String[] derivedProducts = ['datadictionary', 'vocabulary']

env.ONTPUB_FAMILY='fibo'
env.ONTPUB_SPEC_HOST='spec.edmcouncil.org'
env.DEV_SPEC='AboutFIBODev.rdf'
env.PROD_SPEC='AboutFIBOProd.rdf'
env.HYGIENE_TEST_PARAMETER_VALUE='edmcouncil'
env.HYGIENE_WARN_INCONSISTENCY_SPEC_FILE_NAME="${env.DEV_SPEC}"
env.HYGIENE_ERROR_INCONSISTENCY_SPEC_FILE_NAME="${env.PROD_SPEC}"
env.ONTPUB_IS_DARK_MODE='1'
env.ONTPUB_IMAGE='edmcouncil/ontology-publisher:build-dev'
env.LC_ALL='en_US.UTF-8'
env.LANG='en_US.UTF-8'
env.LANGUAGE='en_US.UTF-8'
env.NGINX_SPEC_ROOT="/opt/dev.${env.ONTPUB_SPEC_HOST}"

properties([
  buildDiscarder(
          logRotator(artifactDaysToKeepStr: '', artifactNumToKeepStr: '', daysToKeepStr: '', numToKeepStr: '30')
  ),
  //
  // We let each stage running on each jenkins slave / agent decide what to check out or not
  //
//  skipDefaultCheckout(),
  //
  // Skip stages once the build status has gone to UNSTABLE.
  //
//  skipStagesAfterUnstable(),
  //
  // There must be SOME limit, if it hangs or whatever then that's a bug and therefore cancel the job.
  //
//  timeout(time: 23, unit: 'HOURS'),
  //
  // Prepend all console output generated by the Pipeline run with the time at which the line was emitted
  //
  //timestamps()
//  ansiColor('xterm')
])

@Field private mainSlackMessageObject = null

def init() {
  echo "Initialising slack functions"

  mainSlackMessageObject = slackSend(
    color: "good",
    message: "Started build <${env.BUILD_URL}|Build #${env.BUILD_NUMBER}> on branch ${env.JOB_NAME}",
    botUser: false,
    baseUrl: null,
    teamDomain: 'fibo-edmc'
  )

  if (mainSlackMessageObject == null) {
    echo "ERROR: Could not initialise slack connection"
  } else {
    println ("mainSlackMessageObject: " + mainSlackMessageObject.dump())
  }
}

def send(String color, String message) {

  if (mainSlackMessageObject == null) {
    init()
    if (mainSlackMessageObject == null) {
      slackSend color: color, message: "${message} (no main message)"
      return
    }
  }

  slackSend color: color, message: message, channel: mainSlackMessageObject.threadId, botUser: true
}

def notifyStage() {

  String buildResult = currentBuild.currentResult

  def message="Stage \"${STAGE_NAME}\" finished with status ${buildResult} in <${env.BUILD_URL}|Build #${env.BUILD_NUMBER}> on branch ${env.JOB_NAME}"

  if (buildResult == "SUCCESS") {
    echo "${message}"
    send("good", message)
  }
  else if (buildResult == "FAILURE") {
    echo "ERROR: ${message}"
    send("danger", message)
  }
  else if (buildResult == "UNSTABLE") {
    echo "WARNING: ${message}"
    send("warning", message)
  }
  else {
    echo "ERROR: ${message}"
    send("danger", message)
  }
}

//
// Return a stage object that will get executed in the appropriate ontology-publisher container
//
def runInOntologyPublisherContainer(Map config, Closure body) {

  assert config.shortStageName != null
  assert config.longStageName != null
  assert body != null

  return {
    stage(config.longStageName) {

      echo "runInOntologyPublisherContainer shortStageName:${config.shortStageName}, longStageName:${config.longStageName}"

      def containerName = "${env.JOB_NAME}-${env.NODE_NAME}-${env.EXECUTOR_NUMBER}-${config.shortStageName}".replaceAll(/[^a-zA-Z0-9_.-]/, "_")

      node('docker') {
        echo "Running in stage \"${config.longStageName}\" now"
        sh "rm -rf input"
        unstash "input"
        sh "ls -ld input/*/*"
        sh "rm -rf output"
        if (config.unstashOutputDirs) {
          config.unstashOutputDirs.each { outputDir ->
            unstash "output-${outputDir}"
          }
        }
        if (("${config.shortStageName}" != "ontology") && ("${config.shortStageName}" != "hygiene")) {
          unstash "output-ontology"
        }
        sh "install -d output tmp"
        echo "Launching docker container ${containerName}:"
        try {
          def dockerImage = docker.image(env.ONTPUB_IMAGE)
          dockerImage.pull()
          dockerImage.inside("""
            --read-only
            --name ${containerName}
            --mount type=bind,source=${env.WORKSPACE}/input,target=/input,readonly,consistency=cached
            --mount type=bind,source=${env.WORKSPACE}/output,target=/output,consistency=delegated
            --mount type=bind,source=${env.WORKSPACE}/tmp,target=/var/tmp,consistency=delegated
          """) {
            echo "Running in docker container ${containerName}"
            //
            // Now execute whatever you had in the closure
            //
            body()
          }
        } catch (e) {
          currentBuild.result = "FAILURE"
          echo "Failed stage \"${config.longStageName}\": ${e}"
          throw e
        } finally {
          echo "Finished stage \"${config.longStageName}\" in docker container ${containerName}"
          if (config.archiveArtifacts == true) {
            archiveArtifacts artifacts: "output/${env.ONTPUB_FAMILY}/${config.shortStageName}/**/*.log", fingerprint: true
          }
          // notifyStage()
          deleteDir()
          dir("${workspace}@tmp") {
            deleteDir()
          }
        }
      }
    }
  }
}
//
// Return the stage object that runs the hygiene tests
//
def hygiene() {

  return runInOntologyPublisherContainer(
    shortStageName: 'hygiene',
    longStageName: 'Hygiene Tests',
    archiveArtifacts: true
  ) {
    sh "/publisher/publish.sh hygiene"
    stash name: "output-hygiene", includes: "output/${env.ONTPUB_FAMILY}/hygiene/**", excludes: "output/${env.ONTPUB_FAMILY}/hygiene/**/*.log"
  }
}

//
// Return the stage that produces the output of the given product
//
def product(String product) {

  return runInOntologyPublisherContainer(
    shortStageName: product,
    longStageName: "Build ${product.capitalize()}",
    archiveArtifacts: true
  ) {
    sh "/publisher/publish.sh ${product}"
    stash name: "output-${product}", includes: "output/${env.ONTPUB_FAMILY}/${product}/**", excludes: "output/${env.ONTPUB_FAMILY}/${product}/**/*.log"
  }
}

//
// Return the stage object that runs the publish action
//
def publish(String[] derivedProducts) {

  return runInOntologyPublisherContainer(
    shortStageName: 'publish',
    longStageName: 'Build Final Content',
    unstashOutputDirs: (derivedProducts + 'hygiene')
  ) {
    //
    // Now after all the above is done, make sure we run the final
    // publish step which zips it all up into the output directory
    //
    sh "/publisher/publish.sh publish"
    //
    // Stash the artifacts generated by the publish command
    //
    dir('output') {
      sh "find . -type f -name *.log -delete"
      tar file: "publishable-output.tar.gz", compress: true, overwrite: true
      stash name: "publishable-output", includes: "publishable-output.tar.gz"
    }
  }
}

node {
  ansiColor('xterm') {
    stage('Setup') {
      dir("input/${env.ONTPUB_FAMILY}") {
        checkout scm
      }
      stash name: "input", useDefaultExcludes: false
      deleteDir()
      dir("${workspace}@tmp") {
        deleteDir()
      }
    }

    hygiene().call()

    product('ontology').call()

    stage('Build Derived Products') {
      def parallelStages = derivedProducts.collectEntries { prod ->
        [ "Build ${prod}" : product(prod)]
      }
      parallel(parallelStages)
    }

    publish(derivedProducts).call()

    //
    // Run the publish on the master jenkins agent by just copying all the generated artifacts right into the workspace
    // on master and let NGINX just serve it from there.
    //
    // This workspace will never be "wiped" so it contains all the older versions as well, wiping this workspace
    // will be bad because we would lose all previously published versions
    //
    stage('Publish') {
      node('edmc-master') {
        // setting variables
        script {
	  // Replace all slashes in a branch name with dashes so that we don't mess up the URLs for the ontologies
	  // ... make it all lower case
          BRANCH = sh(returnStdout: true, script: 'if [ -n "${TAG_NAME}" ] ; then echo "${TAG_NAME//\\//-}" | cut -d_ -f 1 ; else echo "${BRANCH_NAME//\\//-}" ; fi').trim().toLowerCase()
          TAG    = sh(returnStdout: true, script: 'if [ -n "${TAG_NAME}" ] ; then echo "${TAG_NAME}" | cut -d_ -f 2 ; else echo "latest" ; fi').trim()
        }
        try {
          echo "Unstashing the publishable-output"
          unstash 'publishable-output'
          echo "Delete product destination directory: \"${env.ONTPUB_FAMILY}/ontology/${BRANCH}/${TAG}\""
          dir("${NGINX_SPEC_ROOT}/${env.ONTPUB_FAMILY}/ontology/${BRANCH}/${TAG}") {
            deleteDir()
          }
          derivedProducts.each { productName ->
            echo "Delete product destination directory: \"${env.ONTPUB_FAMILY}/${productName}/${BRANCH}/${TAG}\""
            dir("${NGINX_SPEC_ROOT}/${env.ONTPUB_FAMILY}/${productName}/${BRANCH}/${TAG}") {
              deleteDir()
            }
          }
          echo "untar 'publishable-output.tar.gz' to ${NGINX_SPEC_ROOT}"
          untar file: "publishable-output.tar.gz", dir: "${NGINX_SPEC_ROOT}"
        } catch (e) {
          currentBuild.result = "FAILURE"
          echo "Failed the \"${STAGE_NAME}\" stage: ${e}"
          throw e
        } finally {
           echo "Build finished!"
           // notifyStage()
           deleteDir()
           dir("${workspace}@tmp") {
             deleteDir()
           }
           dir("${workspace}@script") {
             deleteDir()
           }
        }
      } // end of node('edmc-master')
    } // end of stage "Publish"
      // FIBO-125
      stage('Ontology Viewer Update') {
        node('edmc-master') {
          // setting variables
          script {
	    // Replace all slashes in a branch name with dashes so that we don't mess up the URLs for the ontologies
	    // ... make it all lower case
            BRANCH = sh(returnStdout: true, script: 'if [ -n "${TAG_NAME}" ] ; then echo "${TAG_NAME//\\//-}" | cut -d_ -f 1 ; else echo "${BRANCH_NAME//\\//-}" ; fi').trim().toLowerCase()
            TAG    = sh(returnStdout: true, script: 'if [ -n "${TAG_NAME}" ] ; then echo "${TAG_NAME}" | cut -d_ -f 2 ; else echo "latest" ; fi').trim()
          }
          // sending "/api/update"
          //	REQUIRES credentialsId: "apiKey_${env.ONTPUB_SPEC_HOST}_${env.ONTPUB_FAMILY}_ontology"
          withCredentials([string(credentialsId: "apiKey_${env.ONTPUB_SPEC_HOST}_${env.ONTPUB_FAMILY}_ontology", variable: 'API_KEY')]) {
            def OntologyUpdateApiURL     = "http://fibo-viewer.korora.makolab.net/${env.ONTPUB_FAMILY}/ontology/${BRANCH}/${TAG}/api/update"
            def OntologyUpdateApiHeaders = [[name: "X-API-Key", value: "${API_KEY}"]]
            def response = httpRequest url: OntologyUpdateApiURL, acceptType: 'APPLICATION_JSON', customHeaders: OntologyUpdateApiHeaders, httpMode: 'PUT'
            def ret = response.getStatus()
            if (ret.equals(200)) {
             def json = new JsonSlurper().parseText(response.content)
             def updateId = json.id
             echo "Start \"${OntologyUpdateApiURL}\" update (${response.status}):\n id:\t\t\t${json.id}\n status:\t\t${json.status}\n msg:\t\t\t${json.msg}\n startTimestamp:\t${json.startTimestamp}"
             waitUntil {
              sleep(5)
              response = httpRequest url: "${OntologyUpdateApiURL}/${updateId}", acceptType: 'APPLICATION_JSON', customHeaders: OntologyUpdateApiHeaders
              ret = response.getStatus()
              if (ret.equals(200)) {
               json = new JsonSlurper().parseText(response.content)
               echo "id=${updateId} status=${json.status}"
              }
              ret.equals(200) && json.status != "CREATED" && json.status != "IN_PROGRESS" && json.status != "WAITING" && json.status != "INTERRUPT_IN_PROGRESS"
             }
            }
          }
        }
      } // end of stage "Ontology Viewer Update"
  } // end of ansiColor('xterm')
}

